{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys packages\n",
    "import configparser\n",
    "import getpass\n",
    "import os\n",
    "import sys\n",
    "import psutil\n",
    "\n",
    "# spark\n",
    "import findspark\n",
    "\n",
    "# time\n",
    "import time\n",
    "\n",
    "# Libraries for loading and cleaning the dataset\n",
    "from datetime import datetime\n",
    "import sqlalchemy as sacl\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libaries for Machine Learning Algorithm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Libraries for visualization of results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import pydotplus\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Library for storing and loading objects\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Library for multiprocessing\n",
    "import concurrent.futures\n",
    "\n",
    "cwd=\"C:/Users/OSV/OneDrive - Novo Nordisk/Projects/aom_laad/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font size = \"12\">Loading the dataset</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_laad = create_engine('postgresql://aom_laad_read_user:Laad@123@novonordiskredshift.clo2x7mxqgak.us-east-1.redshift.amazonaws.com:5439/novodb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the laad_aom.icg_patient_vw patient dataset 3 million rows at a time, because it takes too much memory above 3 million\n",
    "# and saving the dataset in a serialized joblib format for later use \n",
    "for i in range(0, 28, 3):\n",
    "    time1 = time.time()\n",
    "    q_str = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM laad_aom.icg_patient_vw\n",
    "    ORDER BY patient_id\n",
    "    OFFSET {i}000000\n",
    "    LIMIT 3000000\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql_query(q_str, engine_laad)\n",
    "    df.set_index('patient_id', inplace = True)\n",
    "    joblib.dump(df, path + f'/df_{i}.joblib')\n",
    "    \n",
    "    print(f'time taken for {i}th million row = {time.time() - time1} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_str2 = \"\"\"\n",
    "SELECT\n",
    "  * \n",
    "FROM laad_aom.icg_prescriber_vw\n",
    "ORDER BY prescriber_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the laad_aom.icg_prescriber_vw table\n",
    "try:\n",
    "    df_prescriber = joblib.load(path + '/Prescriber Flat Table.joblib')\n",
    "except FileNotFoundError:\n",
    "    df_prescriber = pd.read_sql_query(q_str2, engine_laad)\n",
    "    df_prescriber.dropna(subset = ['prescriber_id'], axis = 0, inplace = True)\n",
    "    df_prescriber['prescriber_id'] = df_prescriber['prescriber_id'].astype('int32')\n",
    "    df_prescriber.set_index('prescriber_id', inplace = True)\n",
    "    joblib.dump(df_prescriber, path + '/Prescriber Flat Table.joblib')\n",
    "df_prescriber.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_str3 = \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM laad_aom.icg_plantrak_vw\n",
    "ORDER BY plantrak_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the laad_aom.icg_plantrak_vw table \n",
    "try:\n",
    "    df_plantrak = joblib.load(path + '/Plantrak Flat Table.joblib')\n",
    "except FileNotFoundError:\n",
    "    df_plantrak = pd.read_sql_query(q_str3, engine_laad)\n",
    "    saxenda_plantrak = pd.read_excel(path + '/plantrak_id where total paid claims Saxenda more than 0_excel_version.xlsx')\n",
    "    saxenda_plantrak['joined_plantrak_id'] = saxenda_plantrak['joined_plantrak_id'].apply(lambda x: '00000' + str(x) if len(str(x)) == 5 else ('0000' + str(x) if len(str(x)) == 6 else ('000' + str(x) if len(str(x)) == 7 else ('00' + str(x) if len(str(x)) == 8 else ('0' + str(x) if len(str(x)) == 9 else str(x))))))\n",
    "    saxenda_plantrak['saxenda_plantrak_yn'] = 'Y'\n",
    "    saxenda_plantrak.set_index('joined_plantrak_id', inplace = True)\n",
    "    df_plantrak = df_plantrak.merge(saxenda_plantrak, left_on = 'plantrak_id', right_on = 'joined_plantrak_id', how = 'left')\n",
    "    df_plantrak.fillna({'saxenda_plantrak_yn': 'N'}, inplace = True)\n",
    "    df_plantrak.set_index('plantrak_id', inplace = True)\n",
    "    joblib.dump(df_plantrak, path + '/Plantrak Flat Table.joblib')\n",
    "df_plantrak.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and cleaning the socioecnomic table\n",
    "try:\n",
    "    socioeconomic_df = joblib.load(path + '/socioeconomic data.joblib')\n",
    "except FileNotFoundError:\n",
    "    socioeconomic_df = pd.read_excel(path + '/Final_SED.xlsx')\n",
    "    socioeconomic_df['ZipCode'] = socioeconomic_df['ZipCode'].apply(lambda x: '00' + str(x) if len(str(x)) == 3 else ('0' + str(x) if (len(str(x)) == 4 and str(x) != 'None') else str(x)))\n",
    "    socioeconomic_df.set_index('ZipCode', inplace = True)\n",
    "    \n",
    "    for i in socioeconomic_df.dtypes[socioeconomic_df.dtypes == 'float64'].index:\n",
    "        socioeconomic_df[i] = socioeconomic_df[i].astype('float32')\n",
    "    for i in socioeconomic_df.dtypes[socioeconomic_df.dtypes == 'int64'].index:\n",
    "        socioeconomic_df[i] = socioeconomic_df[i].astype('int32')\n",
    "        \n",
    "    joblib.dump(socioeconomic_df, path + '/socioeconomic data.joblib')\n",
    "socioeconomic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to merge the Patient flat table with the Socioeconomic flat table\n",
    "def merging(df, socioeconomic_df, df_plantrak, zip_name, plantrak_name):\n",
    "    df[zip_name] = df[zip_name].apply(lambda x: '00' + str(x) if len(str(x)) == 3 else ('0' + str(x) if (len(str(x)) == 4 and str(x) != 'None') else str(x)))\n",
    "    \n",
    "    socioeconomic_df.drop(columns = ['State'], inplace = True)\n",
    "    socioeconomic_df['Unemployment Rate'] = pd.to_numeric(socioeconomic_df['Unemployment Rate'], errors = 'coerce')\n",
    "    \n",
    "    df = df.merge(df_plantrak['saxenda_plantrak_yn'], left_on = plantrak_name, right_on = 'plantrak_id', how = 'left')\n",
    "    df = df.merge(socioeconomic_df, left_on = zip_name, right_on = 'ZipCode', how = 'left')\n",
    "    df.set_index('patient_id', inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the paitent flat table with the socioeconomic table and the plantrak table 3 million rows at a time and saving the file \n",
    "# in a serialized joblib format for future use\n",
    "for i in range(0, 28, 3):\n",
    "    time1 = time.time()\n",
    "    df_merge = joblib.load(path + f'/df_{i}.joblib')\n",
    "    df_merge = merging(df_merge, socioeconomic_df, df_plantrak, 'zip', 'joined_plantrak_id')\n",
    "    \n",
    "    joblib.dump(df_merge, path + f'/df_{i}.joblib')\n",
    "    \n",
    "    print(f'time taken for {i}th million row = {time.time() - time1} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = joblib.load(path + '/df_27.joblib')\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_str4 = \"\"\"\n",
    "SELECT\n",
    "  patient_id,\n",
    "  prod_family,\n",
    "  staytime_with_break\n",
    "FROM laad_aom.persist_360_prod_family\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the staytime dataset\n",
    "try:\n",
    "    df_staytime = joblib.load(path + '/df_staytime.joblib')\n",
    "except FileNotFoundError:\n",
    "    df_staytime = pd.read_sql_query(q_str4, engine_laad)\n",
    "    df_staytime['Saxenda staytime'] = np.where(df_staytime['prod_family'] == 'SaxendaÂ® Brand', df_staytime['staytime_with_break'], 0)\n",
    "    df_staytime['Saxenda staytime'] = df_staytime['Saxenda staytime'].astype('int16')\n",
    "\n",
    "    df_staytime['Belviq staytime'] = np.where(df_staytime['prod_family'] == 'Belviq Brand', df_staytime['staytime_with_break'], 0)\n",
    "    df_staytime['Belviq staytime'] = df_staytime['Belviq staytime'].astype('int16')\n",
    "\n",
    "    df_staytime['Contrave staytime'] = np.where(df_staytime['prod_family'] == 'Contrave Brand', df_staytime['staytime_with_break'], 0)\n",
    "    df_staytime['Contrave staytime'] = df_staytime['Contrave staytime'].astype('int16')\n",
    "\n",
    "    df_staytime['Qsymia staytime'] = np.where(df_staytime['prod_family'] == 'Qsymia Brand', df_staytime['staytime_with_break'], 0)\n",
    "    df_staytime['Qsymia staytime'] = df_staytime['Qsymia staytime'].astype('int16')\n",
    "\n",
    "    df_staytime['Generic staytime'] = np.where((df_staytime['prod_family'] == 'Phentermine Franchise') | (df_staytime['prod_family'] == 'Orlistat Franchise'), df_staytime['staytime_with_break'], 0)\n",
    "    df_staytime['Generic staytime'] = df_staytime['Generic staytime'].astype('int16')\n",
    "\n",
    "    df_staytime.drop(columns = ['prod_family', 'staytime_with_break'], inplace = True)\n",
    "    df_staytime.set_index('patient_id', inplace = True)\n",
    "    df_staytime = df_staytime.groupby('patient_id').sum()\n",
    "    \n",
    "    joblib.dump(df_staytime, path + '/df_staytime.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of queries that will help load the laad_aom.icg_patient_vw dataset by columns instead of rows\n",
    "# This allows all 28 million rows to be loaded with a few columns at a time\n",
    "q_str_col_0 = \"\"\"\n",
    "SELECT\n",
    "  patient_id,\n",
    "  patient_birth_year,\n",
    "  patient_gender,\n",
    "  age_during_first_diagnosis,\n",
    "  age_during_latest_diagnosis,\n",
    "  BMI_latest,\n",
    "  \n",
    "  common_wt_cm_dx_yn,\n",
    "  overweight_dx_yn,\n",
    "  any_wt_cm_dx_yn,\n",
    "  obesity_dx_yn,\n",
    "  baom_label_adult_yn,\n",
    "  baom_label_adolescent_yn,\n",
    "  overweight_and_wt_cm_dx_yn,\n",
    "  obesity_or_ow_and_cm_yn\n",
    "FROM laad_aom.icg_patient_vw\n",
    "ORDER BY patient_id\n",
    "\"\"\"\n",
    "\n",
    "q_str_col_1 = \"\"\"\n",
    "SELECT\n",
    "  patient_id, \n",
    "  group_consult_yn,\n",
    "  count_group_consult,\n",
    "  individual_consult_yn,\n",
    "  count_individual_consult,\n",
    "  screening_yn,\n",
    "  count_screening,\n",
    "  surgery_yn,\n",
    "  count_surgery,\n",
    "  first_consult_service_date,\n",
    "  last_consult_service_date,\n",
    "  first_surgery_service_date,\n",
    "  last_surgery_service_date\n",
    "FROM laad_aom.icg_patient_vw\n",
    "ORDER BY patient_id\n",
    "\"\"\"\n",
    "\n",
    "q_str_col_2 = \"\"\"\n",
    "SELECT\n",
    "  patient_id,\n",
    "  total_rx_claims,\n",
    "  total_pd_claims,  \n",
    "  \n",
    "  total_pd_Saxenda_claims,\n",
    "  total_pd_Contrave_claims,\n",
    "  total_pd_Qsymia_claims,\n",
    "  total_pd_Belviq_claims,\n",
    "  total_pd_Generic_claims,  \n",
    "\n",
    "  stdaln_PD_nonlifecycle_claims,  \n",
    "  stdaln_PD_lifecycle_claims,\n",
    "  final_PD_claims,  \n",
    "  stdaln_RJ_nonlifecycle_claims,   \n",
    "  stdaln_RJ_lifecycle_claims,  \n",
    "  stdaln_RV_nonlifecycle_claims,  \n",
    "  stdaln_RV_lifecycle_claims, \n",
    "  initial_RV_claims,  \n",
    "  initial_RJ_claims,  \n",
    "  final_RJ_claims, \n",
    "  final_RV_claims,\n",
    "  \n",
    "  prescribed_Saxenda_yn,\n",
    "  prescribed_other_BRANDED_AOMS_yn,\n",
    "  prescribed_GENERIC_AOMS_yn,\n",
    "  total_opc_saxenda,\n",
    "  avg_opc_saxenda,\n",
    "  total_opc_other_branded_AOMS,\n",
    "  avg_opc_other_branded_AOMS,\n",
    "  total_opc_generic_AOMS,\n",
    "  avg_opc_generic_AOMS\n",
    "FROM laad_aom.icg_patient_vw\n",
    "ORDER BY patient_id\n",
    "\"\"\"\n",
    "\n",
    "q_str_col_3 = \"\"\"\n",
    "SELECT\n",
    "  patient_id,\n",
    "  dx_most_freq_prescriber_id,\n",
    "  dx_most_freq_state,\n",
    "  dx_most_freq_zip,\n",
    "  dx_most_freq_plantrak_id\n",
    "FROM laad_aom.icg_patient_vw\n",
    "ORDER BY patient_id\n",
    "\"\"\"\n",
    "\n",
    "q_str_col_4 = \"\"\"\n",
    "SELECT\n",
    "  patient_id,\n",
    "  first_diagnosis_date,\n",
    "  dx_first_prescriber_id,\n",
    "  dx_first_state,\n",
    "  dx_first_zip,\n",
    "  dx_first_plantrak_id\n",
    "FROM laad_aom.icg_patient_vw\n",
    "ORDER BY patient_id\n",
    "\"\"\"\n",
    "\n",
    "q_str_col_5 = \"\"\"\n",
    "SELECT\n",
    "  patient_id,\n",
    "  latest_diagnosis_date,\n",
    "  dx_latest_prescriber_id,\n",
    "  dx_latest_state,\n",
    "  dx_latest_zip,\n",
    "  dx_latest_plantrak_id\n",
    "FROM laad_aom.icg_patient_vw\n",
    "ORDER BY patient_id\n",
    "\"\"\"\n",
    "\n",
    "q_str_col_6 = \"\"\"\n",
    "SELECT\n",
    "  patient_id,\n",
    "  rx_most_freq_prescriber_id,\n",
    "  rx_most_freq_state,\n",
    "  rx_most_freq_zip,\n",
    "  rx_most_freq_plantrak_id\n",
    "FROM laad_aom.icg_patient_vw\n",
    "ORDER BY patient_id\n",
    "\"\"\"\n",
    "\n",
    "q_str_col_7 = \"\"\"\n",
    "SELECT\n",
    "  patient_id,\n",
    "  first_prescription_date,\n",
    "  first_paid_prescription_date,\n",
    "  rx_first_prescriber_id,\n",
    "  rx_first_prescriber_state,\n",
    "  rx_first_prescriber_zip,\n",
    "  rx_first_plantrak_id,\n",
    "  first_brand_prescribed_Saxenda_yn,\n",
    "  first_brand_prescribed_other_branded_AOMs_yn, \n",
    "  first_brand_prescribed_generic_AOMS_yn\n",
    "FROM laad_aom.icg_patient_vw\n",
    "ORDER BY patient_id\n",
    "\"\"\"\n",
    "\n",
    "q_str_col_8 = \"\"\"\n",
    "SELECT\n",
    "  patient_id,\n",
    "  latest_prescription_date,\n",
    "  latest_paid_prescription_date,\n",
    "  rx_latest_prescriber_id,\n",
    "  rx_latest_prescriber_state,\n",
    "  rx_latest_prescriber_zip,\n",
    "  rx_latest_plantrak_id,\n",
    "  latest_brand_prescribed_Saxenda_yn,\n",
    "  latest_brand_prescribed_other_branded_AOMs_yn, \n",
    "  latest_brand_prescribed_generic_AOMS_yn\n",
    "FROM laad_aom.icg_patient_vw\n",
    "ORDER BY patient_id\n",
    "\"\"\"\n",
    "\n",
    "q_str_col_9 = \"\"\"\n",
    "SELECT\n",
    "  patient_id,\n",
    "  joined_prescriber_id,\n",
    "  nni_saxenda_gsb,\n",
    "  nni_saxenda_target,\n",
    "  zip,\n",
    "  state,\n",
    "  joined_plantrak_id,\n",
    "  method_of_payment,\n",
    "  model_type  \n",
    "FROM laad_aom.icg_patient_vw\n",
    "ORDER BY patient_id\n",
    "\"\"\"\n",
    "\n",
    "q_str_col_10 = \"\"\"\n",
    "SELECT\n",
    "  patient_id,\n",
    "  days_between_first_diag_latest_diag,\n",
    "  days_between_first_consult_latest_consult,\n",
    "  days_between_first_surgery_latest_surgery,\n",
    "  days_between_first_prescr_latest_prescr,\n",
    "  days_between_first_PD_prescr_latest_PD_prescr,\n",
    "  days_between_first_consult_latest_surgery,\n",
    "  days_between_first_diag_latest_prescr,\n",
    "  days_between_first_diag_first_prescr,\n",
    "  days_between_latest_diag_latest_prescr,\n",
    "  days_between_latest_diag_first_prescr\n",
    "FROM laad_aom.icg_patient_vw\n",
    "ORDER BY patient_id\n",
    "\"\"\"\n",
    "\n",
    "q_str_col_list = [q_str_col_0, q_str_col_1, q_str_col_2, q_str_col_3, q_str_col_4, q_str_col_5, \n",
    "                  q_str_col_6, q_str_col_7, q_str_col_8, q_str_col_9, q_str_col_10]\n",
    "patient_subsection_title = ['Patient General Diagnosis info', 'Patient PX data info', 'Patient Medical Claims info', 'Patient Dx Most Frequent Prescriber info',\n",
    "                            'Patient Dx First Prescriber info', 'Patient Dx Latest Prescriber info', 'Patient Rx Most Frequent Prescriber info', \n",
    "                            'Patient Rx First Prescriber info', 'Patient Rx Latest Prescriber info', 'Patient Joined Dx and Rx and Socioeconomic info',\n",
    "                            'Patient Days between dates info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load Patient flat table subsectioned by columns\n",
    "for i in range(11):\n",
    "    time1 = time.time()\n",
    "    socioeconomic_df = joblib.load(path +'/socioeconomic data.joblib')\n",
    "    df_col = pd.read_sql_query(q_str_col_list[i], engine_laad)\n",
    "    if i == 3:\n",
    "        df_col = df_col.merge(df_prescriber[['nni_saxenda_gsb']], left_on = 'dx_most_freq_prescriber_id', right_on = 'prescriber_id', how = 'left')\n",
    "        df_col = merging(df_col, socioeconomic_df, df_plantrak, 'dx_most_freq_zip', 'dx_most_freq_plantrak_id')\n",
    "    elif i == 4:\n",
    "        df_col = df_col.merge(df_prescriber[['nni_saxenda_gsb']], left_on = 'dx_first_prescriber_id', right_on = 'prescriber_id', how = 'left')\n",
    "        df_col = merging(df_col, socioeconomic_df, df_plantrak, 'dx_first_zip', 'dx_first_plantrak_id')\n",
    "    elif i == 5:\n",
    "        df_col = df_col.merge(df_prescriber[['nni_saxenda_gsb']], left_on = 'dx_latest_prescriber_id', right_on = 'prescriber_id', how = 'left')\n",
    "        df_col = merging(df_col, socioeconomic_df, df_plantrak, 'dx_latest_zip', 'dx_latest_plantrak_id')\n",
    "    elif i == 6:\n",
    "        df_col = df_col.merge(df_prescriber[['nni_saxenda_gsb']], left_on = 'rx_most_freq_prescriber_id', right_on = 'prescriber_id', how = 'left')\n",
    "        df_col = merging(df_col, socioeconomic_df, df_plantrak, 'rx_most_freq_zip', 'rx_most_freq_plantrak_id')\n",
    "    elif i == 7:\n",
    "        df_col = df_col.merge(df_prescriber[['nni_saxenda_gsb']], left_on = 'rx_first_prescriber_id', right_on = 'prescriber_id', how = 'left')\n",
    "        df_col = merging(df_col, socioeconomic_df, df_plantrak, 'rx_first_prescriber_zip', 'rx_first_plantrak_id')\n",
    "    elif i == 8:\n",
    "        df_col = df_col.merge(df_prescriber[['nni_saxenda_gsb']], left_on = 'rx_latest_prescriber_id', right_on = 'prescriber_id', how = 'left')\n",
    "        df_col = merging(df_col, socioeconomic_df, df_plantrak, 'rx_latest_prescriber_zip', 'rx_latest_plantrak_id')\n",
    "    elif i == 9:\n",
    "        df_col = merging(df_col, socioeconomic_df, df_plantrak, 'zip', 'joined_plantrak_id')\n",
    "    else:\n",
    "        df_col.set_index('patient_id', inplace = True)\n",
    "    print(f'Time taken for q_str_col_{i} = {time.time() - time1} seconds')\n",
    "    joblib.dump(df_col, path + f'/Patient flat table Subsections/{patient_subsection_title[i]}.joblib')\n",
    "    print(psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font size = \"12\">Cleaning and Scaling the dataset</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to clean and organize the raw data before fitting the clustering algorithm\n",
    "def data_cleaning(df_merge):\n",
    "    \n",
    "    # User-defined functions to be used in later parts of the data cleaning\n",
    "    def age_latest_diag(age_latest_diag, birth_year, latest_prescription_date):\n",
    "        if age_latest_diag == -5:\n",
    "            return latest_prescription_date.year - birth_year\n",
    "        else:\n",
    "            return age_latest_diag \n",
    "    \n",
    "    def baom_adult_yn(baom_label_adult, age_during_latest_diagnosis):\n",
    "        if baom_label_adult == -5:\n",
    "            if age_during_latest_diagnosis >= 17:\n",
    "                return 'Y'\n",
    "            else:\n",
    "                return 'N'\n",
    "        else:\n",
    "            return baom_label_adult\n",
    "    \n",
    "    # Adding a new column called total_pd_branded_aoms to show all total paid claims for branded aoms\n",
    "    df_merge['total_pd_branded_aom_claims'] = df_merge['total_pd_claims'] - df_merge['total_pd_generic_claims']\n",
    "    \n",
    "    # Adding two new columns that show whether the patient is in the Dx or Rx database or both databases\n",
    "    df_merge['In Dx database_yn'] = np.where(np.isnan(df_merge['dx_most_freq_prescriber_id']), 'N', 'Y')\n",
    "    df_merge['In Rx database_yn'] = np.where(np.isnan(df_merge['rx_most_freq_prescriber_id']), 'N', 'Y')\n",
    "    \n",
    "    # Creating a copy of df_merge with NaN values filled with a dummy value because apply methods\n",
    "    # do not work with dataframes that has NaN values\n",
    "    # this dataframe is used when necessary\n",
    "    df_temp = df_merge.fillna(-5)\n",
    "    \n",
    "    # Business decision: Since patients in Rx database have null values in latest diagnosis columns,\n",
    "    # we assume that the age during latest diagnosis is the difference between age of their latest prescription\n",
    "    # and birth year\n",
    "    df_merge['age_during_latest_diagnosis'] = df_temp.apply(lambda x: age_latest_diag(x['age_during_latest_diagnosis'], x['patient_birth_year'], \n",
    "                                                                                      x['latest_prescription_date']), axis = 1)\n",
    "    df_temp['age_during_latest_diagnosis'] = df_merge['age_during_latest_diagnosis']\n",
    "    \n",
    "\n",
    "    # Choosing the selected columns in a list and dropping columns not in this list\n",
    "    Selected_columns = np.array(['patient_birth_year', 'patient_gender', 'age_during_latest_diagnosis', 'bmi_latest', 'any_wt_cm_dx_yn', 'obesity_dx_yn',\n",
    "                    'baom_label_adult_yn', 'obesity_or_ow_and_cm_yn', 'count_group_consult', 'count_individual_consult', \n",
    "                    'count_screening', 'count_surgery', 'total_rx_claims', 'total_pd_claims', 'total_pd_saxenda_claims', \n",
    "                    'total_pd_contrave_claims', 'total_pd_qsymia_claims', 'total_pd_belviq_claims', 'total_pd_generic_claims', \n",
    "                   'stdaln_pd_nonlifecycle_claims', 'stdaln_pd_lifecycle_claims', 'final_pd_claims', 'stdaln_rj_nonlifecycle_claims',\n",
    "                   'stdaln_rj_lifecycle_claims', 'stdaln_rv_nonlifecycle_claims', 'stdaln_rv_lifecycle_claims', 'initial_rv_claims',\n",
    "                   'initial_rj_claims', 'final_rj_claims', 'final_rv_claims', 'prescribed_saxenda_yn', 'prescribed_other_branded_aoms_yn',\n",
    "                   'prescribed_generic_aoms_yn', 'avg_opc_saxenda', 'avg_opc_other_branded_aoms', 'avg_opc_generic_aoms', 'nni_saxenda_gsb',\n",
    "                   'saxenda_plantrak_yn','days_between_first_diag_latest_diag', 'days_between_first_pd_prescr_latest_pd_prescr', \n",
    "                   \"% Bachelor's\", '% HS diploma', '% less than HS diploma', '% post grad', '% college', 'Average HH size', 'Median gross rent',\n",
    "                   'Median value of an owner-occupied home', 'Median HH income', 'Median owner cost burden', 'Median renter cost burden', \n",
    "                   '% Asian', '% Black', '% Hispanic', '% White', 'Population', 'Unemployment Rate', 'In Dx database_yn', 'In Rx database_yn'])\n",
    "    \n",
    "    Dropped_columns = list()\n",
    "    \n",
    "    for i in df_merge.columns:\n",
    "        if i not in Selected_columns:\n",
    "            Dropped_columns.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Creating a separate dataframe, called df_cluster that is separated from the original dataframe, df_merge\n",
    "    df_cluster = df_merge.drop(columns = Dropped_columns)\n",
    "    \n",
    "    # Changing patient birth year to age because that will yield better results in our model\n",
    "    df_cluster['patient_birth_year'] = df_cluster['patient_birth_year'].apply(lambda x: datetime.now().year - x)\n",
    "    df_cluster.rename(columns = {'patient_birth_year': 'current_age'}, inplace = True)\n",
    "    \n",
    "    # Dropping patients with gender 'U'\n",
    "    df_cluster = df_cluster[df_cluster['patient_gender'] != 'U']\n",
    "    \n",
    "    # Filling the null values of column 'baom_label_adult_yn' with Y/N values based on the patient's current age\n",
    "    df_cluster['baom_label_adult_yn'] = df_temp.apply(lambda x: baom_adult_yn(x['baom_label_adult_yn'], x['age_during_latest_diagnosis']), axis = 1)\n",
    "    \n",
    "    # Translating the categorical variable nni_saxenda_gsb to a numerical variable that showcases ranking of each prescriber\n",
    "    df_cluster['nni_saxenda_gsb'] = df_cluster['nni_saxenda_gsb'].map({'Tier 1': 10, 'Tier 2': 8, 'Tier 3': 6, 'No Tier': 2, np.nan: 0}).astype('int8')\n",
    "    \n",
    "    # Performing Label encoding on categorial variable: patient gender and bmi_latest\n",
    "    df_cluster['patient_gender'] = df_cluster['patient_gender'].map({'M': 1, 'F': 0}).astype('int8')\n",
    "    df_cluster['bmi_latest'] = df_cluster['bmi_latest'].map({'>30': 1, '27': 0, np.nan: 0}).astype('int8')\n",
    "    \n",
    "    # Dropping NaN values for socioeconomic data\n",
    "    df_cluster.dropna(subset = ['Unemployment Rate'], axis = 0, inplace = True)\n",
    "    \n",
    "    # Peforming Label encoding on 'Y'/'N' categorical variables\n",
    "    for i in df_cluster.dtypes[df_cluster.dtypes == 'O'].index:\n",
    "        df_cluster[i] = df_cluster[i].map({'Y': 1, 'N': 0, np.nan: 0}).astype('int8')\n",
    "        \n",
    "    # Replacing every other NaN value with 0\n",
    "    df_cluster.fillna(int(0), inplace = True)\n",
    "    \n",
    "    # This section of code converts every datatype to one that minimuzes memory usage\n",
    "    # Columns selected from current age to obesity_or_ow_and_cm_yn\n",
    "    for i in df_cluster.columns[0:8]:\n",
    "        df_cluster[i] = df_cluster[i].astype('int8')\n",
    "\n",
    "    # Columns selected from count_group_consult to final_rv_claims\n",
    "    for i in df_cluster.columns[8:30]:\n",
    "        df_cluster[i] = df_cluster[i].astype('int16')\n",
    "\n",
    "    # Columns selected from prescribed_saxenda_yn to prescribed_generic_aoms_yn\n",
    "    for i in df_cluster.columns[30:33]:\n",
    "        df_cluster[i] = df_cluster[i].astype('int8')\n",
    "\n",
    "    # Columns selected from avg_opc_saxenda to avg_opc_generic_aoms\n",
    "    for i in df_cluster.columns[33:36]:\n",
    "        df_cluster[i] = df_cluster[i].astype('float32')\n",
    "\n",
    "    df_cluster['nni_saxenda_gsb'] = df_cluster['nni_saxenda_gsb'].astype('int8')\n",
    "\n",
    "    # Columns selected from days between first and latest diagnosis to days between first and latest paid prescription\n",
    "    for i in df_cluster.columns[37:39]:\n",
    "        df_cluster[i] = df_cluster[i].astype('int16')\n",
    "\n",
    "    # Columns selected from % Bachelor's to % White\n",
    "    for i in df_cluster.columns[39:54]:\n",
    "        df_cluster[i] = df_cluster[i].astype('float32')\n",
    "\n",
    "    df_cluster['Population'] = df_cluster['Population'].astype('int32')\n",
    "\n",
    "    df_cluster['Unemployment Rate'] = df_cluster['Unemployment Rate'].astype('float32')\n",
    "\n",
    "    # Columns selected from saxenda_plantrak_yn to In Rx_database_yn\n",
    "    for i in df_cluster.columns[56:59]:\n",
    "        df_cluster[i] = df_cluster[i].astype('int8')\n",
    "    \n",
    "    # Delete object to free up memory\n",
    "    del df_temp\n",
    "    \n",
    "    return df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the patient dataset that is loaded 3 million rows at a time\n",
    "for i in range(0, 28, 3):\n",
    "    time1 = time.time()\n",
    "    df_cluster = joblib.load(path + f'/df_{i}.joblib')\n",
    "    df_cluster = data_cleaning(df_cluster)\n",
    "    \n",
    "    joblib.dump(df_cluster, path + f'/df_{i}.joblib')\n",
    "    \n",
    "    print(f'time taken for {i}th million row = {time.time() - time1} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = joblib.load(path + '/df_27 cleaned.joblib')\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading each 3 million cleaned patient dataset, and then comibining them to build an entire 28 million patient dataset that is cleaned\n",
    "df_cluster_list = list()\n",
    "for i in range(0, 28, 3):\n",
    "    df_cluster_list.append(joblib.load(path + f'/df_{i}.joblib'))\n",
    "\n",
    "df_cluster = pd.concat(df_cluster_list)\n",
    "joblib.dump(df_cluster, path + '/df_cleaned.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading df_cluster which is the cleaned data from joblib\n",
    "df_cluster = joblib.load(path + '/df_cleaned.joblib')\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to scale the continuous variables in our data before fitting our model in it, in order to account\n",
    "# for the different weights of each dimension\n",
    "def scaling(df_cluster):\n",
    "    \n",
    "    categorical_variables = ['patient_gender', 'bmi_latest', 'any_wt_cm_dx_yn', 'obesity_dx_yn', 'baom_label_adult_yn', \n",
    "                             'obesity_or_ow_and_cm_yn', 'prescribed_saxenda_yn', 'prescribed_other_branded_aoms_yn', 'prescribed_generic_aoms_yn',\n",
    "                             'saxenda_plantrak_yn', 'In Dx database_yn', 'In Rx database_yn']\n",
    "\n",
    "    # Taking out the categorical variables first before scaling\n",
    "    df_categorical = df_cluster.loc[:, categorical_variables]\n",
    "\n",
    "    continuous_variables = list()\n",
    "\n",
    "    for i in df_cluster.columns:\n",
    "        if i not in categorical_variables:\n",
    "            continuous_variables.append(i)\n",
    "\n",
    "    # Create a dataframe containing only continuous variables\n",
    "    df_continuous = df_cluster.loc[:, continuous_variables]\n",
    "\n",
    "    # Scaling the df_continuous dataframe to standardize the values in each column\n",
    "    df_continuous = pd.DataFrame(StandardScaler().fit_transform(df_continuous), columns = df_continuous.columns, index = df_continuous.index)\n",
    "    \n",
    "    # Merging the scaled continuous dataframe and the categorical dataframe to create a scaled cluster dataframe\n",
    "    df_cluster = df_continuous.merge(df_categorical, left_on = 'patient_id', right_on = 'patient_id', how = 'left')\n",
    "    \n",
    "    del df_categorical\n",
    "    del df_continuous\n",
    "    \n",
    "    return df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "df_cluster = scaling(df_cluster)\n",
    "time2 = time.time()\n",
    "print('time taken =', time2 - time1)\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font size = \"12\">Fitting the Clustering Algorithm to the dataset</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to determine the optimal Explained Variance for our Principal Component Analysis (PCA)\n",
    "def pca_variance(df):\n",
    "    \n",
    "    num_components = list()\n",
    "    \n",
    "    variance_choice = np.arange(0.80, 1.00, 0.05)\n",
    "    \n",
    "    for i in variance_choice:\n",
    "        pca = PCA(i)\n",
    "        pca.fit(df)\n",
    "        num_components.append(pca.n_components_)\n",
    "        \n",
    "    %matplotlib inline\n",
    "    sns.set() # Using seaborn style\n",
    "    plt.figure()\n",
    "    plt.plot(variance_choice, num_components)\n",
    "    plt.xlabel('Explained Variance')\n",
    "    plt.ylabel('Number of dimensions')\n",
    "    plt.show() # Plotting the elbow plot\n",
    "    \n",
    "    return num_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = pca_variance(df_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to do PCA on the data to reduce colinearity and create independent\n",
    "# dimensions to speed up the clustering process and increase the accuracy of the model\n",
    "def pca_algorithm(df_scaled):\n",
    "    pca = PCA(0.95) # Get at least a 95% variance\n",
    "    pca.fit(df_scaled)\n",
    "    \n",
    "    df_scaled = pd.DataFrame(data = pca.transform(df_scaled), index = df_scaled.index, columns = ['principal column ' + str(i + 1) for i in range(pca.n_components_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "pca_algorithm(df_cluster)\n",
    "time2 = time.time()\n",
    "print('time taken =', time2 - time1)\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing on a 80-20 split\n",
    "df_train, df_test = train_test_split(df_cluster, test_size = 0.2)\n",
    "joblib.dump(df_train, path + '/df_training.joblib')\n",
    "joblib.dump(df_test, path + '/df_testing.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = joblib.load(path + '/df_training.joblib')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to get the Standard Summation Error (SSE) for a specified number of clusters \n",
    "def kmeans_sse(df_k):\n",
    "    time1 = time.time()\n",
    "    kmeans = KMeans(n_clusters = df_k[1])\n",
    "    kmeans.fit(df_k[0])\n",
    "    print(f'{df_k[1]} clusters take {time.time() - time1} seconds')\n",
    "    sse = kmeans.inertia_\n",
    "    print(psutil.virtual_memory())\n",
    "    if df_k[2] == 'Y':\n",
    "        joblib.dump(sse, path + f'/SSE for KMeans/sse_{df_k[1]}.joblib')\n",
    "    return sse\n",
    "\n",
    "# Function used to plot the elbow plot for KMeans clustering to show if K-Means is a good model for our\n",
    "# data and to figure out the optimal number of clusters to fit our data\n",
    "def elbowplot(sse, start, end):\n",
    "    k_range = range(start, end)\n",
    "    \n",
    "    %matplotlib inline\n",
    "    sns.set() # Using seaborn style\n",
    "    plt.figure(figsize = (8, 8))\n",
    "    plt.plot(k_range, sse)\n",
    "    plt.xlabel('K values')\n",
    "    plt.ylabel('Standard Summation Error')\n",
    "    plt.show() # Plotting the elbow plot\n",
    "    \n",
    "# Function used to fit the KMeans model to the dataframe and return the model\n",
    "def kmeans_alg(components, df):\n",
    "    kmeans = KMeans(n_clusters = components)\n",
    "    predicted_cluster = kmeans.fit_predict(df) + 1\n",
    "    \n",
    "    df_dict = pd.Series(dict(zip(df.index, predicted_cluster)), name = 'Predicted Group')\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using multiprocessing to speed up the computation process\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = 2) as executor:\n",
    "        sse = list(executor.map(kmeans_sse, [(df_train, k, 'Y') for k in range(1, 16)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = [joblib.load(path + f'/SSE for KMeans/sse_{i}.joblib') for i in range(1, 16)]\n",
    "elbowplot(sse, 1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to get the Bayesian Information Criterion (BIC) score for a specified number of components\n",
    "def bic_score(df_comp):\n",
    "    time1 = time.time()\n",
    "    gmm = GaussianMixture(n_components = df_comp[1], random_state = 42, covariance_type = 'full')\n",
    "    gmm.fit(df_comp[0])\n",
    "    print(f'{df_comp[1]} components take {time.time() - time1} seconds')\n",
    "    score = gmm.bic(df_comp[0])\n",
    "    print(psutil.virtual_memory())\n",
    "    if df_comp[2] == 'Y':\n",
    "        joblib.dump(score, path + f'/BIC scores/BIC_score{df_comp[1]}.joblib')\n",
    "    return score\n",
    "\n",
    "# Function used to plot the BIC scores with the number of components, as well as the slope of the gradient of BIC scores with \n",
    "# number of components to help decide the optimal number of clusters that the model can fit to give the most accurate results \n",
    "# while minmizing overfitting the data as best as possible\n",
    "def bic_plot(bic_list, start, end):\n",
    "    component_range = range(start, end)\n",
    "    \n",
    "    gradient = list()\n",
    "        \n",
    "    for i in component_range:\n",
    "        if i == end - 1:\n",
    "            slope = 0\n",
    "        else:\n",
    "            slope = (bic_list[i] - bic_list[i - 1])\n",
    "        gradient.append(slope)\n",
    "    \n",
    "    ROW_NUM = 1\n",
    "    COL_NUM = 2\n",
    "    \n",
    "    %matplotlib inline\n",
    "    sns.set()\n",
    "    fig, (ax1, ax2) = plt.subplots(ROW_NUM, COL_NUM, figsize = (8, 8))\n",
    "    \n",
    "    ax1.plot(component_range, bic_list)\n",
    "    ax1.set_xlabel('number of components')\n",
    "    ax1.set_ylabel('BIC score')\n",
    "    ax1.xaxis.set_ticks(np.arange(start, end, 2))\n",
    "    ax1.set_title('BIC curve VS number of components')\n",
    "    \n",
    "    ax2.plot(component_range, gradient)\n",
    "    ax2.set_xlabel('number of components')\n",
    "    ax2.set_ylabel('Gradient of BIC score')\n",
    "    ax2.xaxis.set_ticks(np.arange(start, end, 2))\n",
    "    ax2.set_title('Gradient of BIC curve VS number of components')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing with 2 processors from components ranging from 1 to 12\n",
    "# Memory starts failing after 12 components, and therefore have to switch to 1 processor for components ranging from 13 to 15\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = 2) as executor:\n",
    "        bic_list = list(executor.map(bic_score, [(df_train, i, 'Y') for i in range(1, 13)]))\n",
    "\n",
    "bic_list = list(map(bic_score, [(df_train, i, 'Y') for i in range(13, 16)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_score_list = [joblib.load(path + f'/BIC scores/BIC_score{i}.joblib') for i in range(1, 16)]\n",
    "bic_plot(bic_score_list, 1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to fit the Gaussian Mixture Model to the dataframe and return the machine learning model\n",
    "def gmm_alg(components, df):\n",
    "    gmm = GaussianMixture(n_components = components, random_state = 42, covariance_type = 'full')\n",
    "    gmm.fit(df)\n",
    "\n",
    "    return gmm\n",
    "\n",
    "# Function used to return a dictionary of the Patient ID and the respective cluster each patient belongs to\n",
    "def gmm_predict(gmm, df_train, df_test):\n",
    "    df_full = pd.concat([df_train, df_test])\n",
    "    predicted_cluster = gmm.predict(df_full) + 1\n",
    "    \n",
    "    df_cluster_dict = pd.Series(dict(zip(np.concatenate([df_train.index, df_test.index]), predicted_cluster)), name = 'Predicted Cluster', dtype = 'int8')\n",
    "    \n",
    "    # Changing the cluster names based on their ranking after visualization\n",
    "    df_cluster_dict = df_cluster_dict.map({1: 1, 4: 2, 3: 3, 5: 4, 2: 5})\n",
    "    \n",
    "    del df_full\n",
    "    del df_train\n",
    "    del df_test\n",
    "    \n",
    "    return df_cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = joblib.load(path + '/df_testing.joblib')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = gmm_alg(5, df_train)       \n",
    "# Dumping the model into bytes so it is saved and ready for future use\n",
    "joblib.dump(gmm, path + '/Gaussian Mixture Model for Clustering Patients.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = joblib.load(path + '/Gaussian Mixture Model for Clustering Patients.joblib')\n",
    "df_cluster_dict = gmm_predict(gmm, df_train, df_test)\n",
    "joblib.dump(df_cluster_dict, path + '/Patient ID and cluster group dictionary.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_dict = joblib.load(path + '/Patient ID and cluster group dictionary.joblib')\n",
    "df_cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_dict.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font size = \"12\">Patient Level Results</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to save and append the predicted clusters to the appropriate dataframes for visualization analysis\n",
    "# as well as creating the 'mean' and 'median' datasets of the clusters for visualization purposes\n",
    "def results(df_cluster_dict, df_cluster):\n",
    "    \n",
    "    # Appending the predicted cluster to a new column in df_cluster\n",
    "    df_cluster['Predicted Cluster'] = df_cluster_dict\n",
    "    \n",
    "    # Getting the means for columns in df_cluster for visualization purposes\n",
    "    df_cluster_means = df_cluster.groupby('Predicted Cluster').mean()\n",
    "    \n",
    "    # Getting the median for columns in df_cluster for visualization purposes\n",
    "    df_cluster_median = df_cluster.groupby('Predicted Cluster').median()\n",
    "    \n",
    "    # Converting these dataframes into bytes so they can be loaded faster in python\n",
    "    joblib.dump(df_cluster_means, path + '/df_cluster_means.joblib')\n",
    "    joblib.dump(df_cluster_median, path + '/df_cluster_median.joblib')\n",
    "    \n",
    "    return df_cluster_means, df_cluster_median\n",
    "\n",
    "# Function used to covert the dataframes to csv or excel\n",
    "def conversion(df, name_of_file, excel = False):\n",
    "    if excel == True:\n",
    "        df.to_excel(path + f'/{name_of_file}.xlsx')\n",
    "    else:\n",
    "        df.to_csv(path + f'/{name_of_file}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to update the patient flat table subsections with the Predicted Cluster\n",
    "for i in os.listdir(path + '/Patient flat table Subsections'):\n",
    "    time1 = time.time()\n",
    "    if i[0:7] == 'Patient':\n",
    "        df_patient = joblib.load(path + f'/Patient flat table Subsections/{i}')\n",
    "        df_patient['Predicted Cluster'] = df_cluster_dict\n",
    "        df_patient['Predicted Cluster'] = df_patient['Predicted Cluster'].fillna(0).astype('int8')\n",
    "        joblib.dump(df_patient, path + f'/Patient flat table Subsections/{i}')\n",
    "    else:\n",
    "        continue\n",
    "    print(f'time taken for {i} = {time.time() - time1} seconds') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = joblib.load(path + '/df_cleaned.joblib')\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_means, df_cluster_median = results(df_cluster_dict, df_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_means = joblib.load(path + '/df_cluster_means.joblib')\n",
    "df_cluster_median = joblib.load(path + '/df_cluster_median.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the mean and median datafiles to csv format\n",
    "conversion(df_cluster_means, 'icg_28 Mil Patient Cluster averages', excel = False)\n",
    "conversion(df_cluster_median, 'icg_28 Mil Patient Cluster medians', excel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV conversion of the Patient flat table Subsections\n",
    "for i in os.listdir(path + '/Patient flat table Subsections'):\n",
    "    time1 = time.time()\n",
    "    if i[0:7] == 'Patient':\n",
    "        df_patient = joblib.load(path + f'/Patient flat table Subsections/{i}')\n",
    "        conversion(df_patient, f'/Patient flat table Subsections/{i[0:-7]}', excel = False)\n",
    "    else:\n",
    "        continue\n",
    "    print(f'time taken for csv conversion of {i} = {time.time() - time1} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.groupby('Predicted Cluster').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_means = joblib.load(path + '/df_cluster_means.joblib')\n",
    "df_cluster_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_median = joblib.load(path + '/df_cluster_median.joblib')\n",
    "df_cluster_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the patient predicted cluster to the staytime dataset\n",
    "df_staytime = joblib.load(path + '/df_staytime.joblib')\n",
    "df_staytime['Predicted Cluster'] = df_cluster_dict\n",
    "df_staytime['Predicted Cluster'] = df_staytime['Predicted Cluster'].fillna(0).astype('int8')\n",
    "df_staytime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the 'mean' values of staytime for each AOM based on each patient cluster\n",
    "staytime_mean = df_staytime.groupby('Predicted Cluster').mean()\n",
    "staytime_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the mean staytime data to csv format\n",
    "conversion(staytime_mean, 'staytime means', excel = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font size = \"12\">ZipCode Level Results</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = joblib.load(path + '/df_cleaned.joblib')\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to initialize and create patient flat table with zipcode as the focus\n",
    "def initialize_zip_level_patients(df_cleaned):\n",
    "    df_cleaned['total_pd_BAOM_claims'] = df_cleaned['total_pd_claims'] - df_cleaned['total_pd_generic_claims']\n",
    "    \n",
    "    # Choosing these specific columns for zip code level analysis\n",
    "    df_patient_zip_level = df_cleaned[['total_pd_generic_claims', 'total_pd_BAOM_claims', 'total_pd_saxenda_claims', 'nni_saxenda_gsb', 'saxenda_plantrak_yn', \n",
    "                                       'obesity_dx_yn', 'obesity_or_ow_and_cm_yn', 'Average HH size', 'Median HH income', 'Median gross rent', '% less than HS diploma', \n",
    "                                       '% HS diploma', '% college', \"% Bachelor's\", '% post grad', 'Unemployment Rate', 'Population']]\n",
    "    \n",
    "    joblib.dump(df_patient_zip_level, path + '/df_patient_zip_level.joblib')\n",
    "    \n",
    "    del df_cleaned\n",
    "    \n",
    "    return df_patient_zip_level\n",
    "\n",
    "# Function used to add columns from socioeconomic data to zipcode level patients\n",
    "def manipulate_zip_level_patients(df_patient_zip_level, df_cluster_dict):\n",
    "    df_joined_dx_rx_socio = joblib.load(path + '/Patient flat table Subsections/Patient Joined Dx and Rx and Socioeconomic info.joblib')\n",
    "    df_patient_zip_level = df_patient_zip_level.merge(df_joined_dx_rx_socio[['zip', 'state', 'Total population of state']], left_on = 'patient_id', right_on = df_joined_dx_rx_socio.index, how = 'left')\n",
    "    df_patient_zip_level['Total population of state'] = df_patient_zip_level['Total population of state'].astype('int32')\n",
    "    \n",
    "    df_patient_zip_level.set_index('patient_id', inplace = True)\n",
    "    df_patient_zip_level['Predicted Cluster'] = df_cluster_dict\n",
    "    \n",
    "    df_patient_zip_level['nni_saxenda_gsb'] = df_patient_zip_level['nni_saxenda_gsb'].map({10: 'Tier 1', 8: 'Tier 2', 6: 'Tier 3', 2: 'No Tier', 0: 'Non-Tier'})\n",
    "    joblib.dump(df_patient_zip_level, path + '/df_patient_zip_level.joblib')\n",
    "    \n",
    "    del df_joined_dx_rx_socio\n",
    "    \n",
    "    return df_patient_zip_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient_zip_level = initialize_zip_level_patients(df_cleaned)\n",
    "df_patient_zip_level.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient_zip_level = manipulate_zip_level_patients(df_patient_zip_level, df_cluster_dict)\n",
    "df_patient_zip_level.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient_zip_level = joblib.load(path + '/df_patient_zip_level.joblib')\n",
    "df_patient_zip_level.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient_zip_level.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_group = df_patient_zip_level.groupby('zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to create the zipcode level dataframe containing some statistics of patients in that zipcode\n",
    "def zipcode_df(zipcode_group):\n",
    "    \n",
    "    # Columns that were decided to be chosen for our zip code flat table\n",
    "    columns_of_zip_df = ['zipcode',\n",
    "                  'num_of_patients',\n",
    "                  'total_pd_generic_claims',\n",
    "                  'total_pd_BAOM_claims',\n",
    "                  'total_pd_saxenda_claims',\n",
    "                  'num_of_non_tiers',\n",
    "                  'num_of_tier_1s',\n",
    "                  'num_of_tier_2s',\n",
    "                  'num_of_tier_3s',\n",
    "                  'num_of_no_tiers',\n",
    "                  'num_of_saxenda_plantrak',\n",
    "                  'num_of_obese_patients',\n",
    "                  'num_of_cormobidity_patients',\n",
    "                  'average_hh_size',\n",
    "                  'median_hh_income',\n",
    "                  'median_gross_rent',\n",
    "                  '% less_than_HS_diploma',\n",
    "                  '% HS_diploma',\n",
    "                  '% college',\n",
    "                  \"% Bachelor's\",\n",
    "                  '% post_grad',\n",
    "                  'unemployment_rate',\n",
    "                  'population',\n",
    "                  'state',\n",
    "                  'popln_of_state',\n",
    "                  'num_of_cluster1_patients',\n",
    "                  'num_of_cluster2_patients',\n",
    "                  'num_of_cluster3_patients',\n",
    "                  'num_of_cluster4_patients',\n",
    "                  'num_of_cluster5_patients']\n",
    "    \n",
    "    df_zip = pd.DataFrame(columns = columns_of_zip_df)  \n",
    "    \n",
    "    for key, value in zipcode_group:\n",
    "        df = zipcode_group.get_group(key)\n",
    "        \n",
    "        tier_list = ['Non-Tier', 'Tier 1', 'Tier 2', 'Tier 3', 'No Tier']\n",
    "        for i in df['nni_saxenda_gsb'].value_counts().index:\n",
    "            for j in range(len(tier_list)):\n",
    "                if i == tier_list[j]:\n",
    "                    tier_list[j] = df['nni_saxenda_gsb'].value_counts()[i]\n",
    "\n",
    "        for i in range(len(tier_list)):\n",
    "            if type(tier_list[i]) == str:\n",
    "                tier_list[i] = 0\n",
    "        \n",
    "        cluster_list = ['1', '2', '3', '4', '5']\n",
    "        for i in df['Predicted Cluster'].value_counts().index:\n",
    "            for j in range(len(cluster_list)):\n",
    "                if i == int(cluster_list[j]):\n",
    "                    cluster_list[j] = df['Predicted Cluster'].value_counts()[i]\n",
    "\n",
    "        for i in range(len(cluster_list)):\n",
    "            if type(cluster_list[i]) == str:\n",
    "                cluster_list[i] = 0\n",
    "                \n",
    "        zip_dictionary = {'zipcode': key,\n",
    "                  'num_of_patients': len(df),\n",
    "                  'total_pd_generic_claims': int(df.sum()['total_pd_generic_claims']),\n",
    "                  'total_pd_BAOM_claims': int(df.sum()['total_pd_BAOM_claims']),\n",
    "                  'total_pd_saxenda_claims': int(df.sum()['total_pd_saxenda_claims']),\n",
    "                  'num_of_non_tiers': tier_list[0],\n",
    "                  'num_of_tier_1s': tier_list[1],\n",
    "                  'num_of_tier_2s': tier_list[2],\n",
    "                  'num_of_tier_3s': tier_list[3],\n",
    "                  'num_of_no_tiers': tier_list[4],\n",
    "                  'num_of_saxenda_plantrak': int(df.sum()['saxenda_plantrak_yn']),\n",
    "                  'num_of_obese_patients': int(df.sum()['obesity_dx_yn']),\n",
    "                  'num_of_cormobidity_patients': int(df.sum()['obesity_or_ow_and_cm_yn']),\n",
    "                  'average_hh_size': df['Average HH size'].iloc[0],\n",
    "                  'median_hh_income': df['Median HH income'].iloc[0],\n",
    "                  'median_gross_rent': df['Median gross rent'].iloc[0],\n",
    "                  '% less_than_HS_diploma': df['% less than HS diploma'].iloc[0],\n",
    "                  '% HS_diploma': df['% HS diploma'].iloc[0],\n",
    "                  '% college': df['% college'].iloc[0],\n",
    "                  \"% Bachelor's\": df[\"% Bachelor's\"].iloc[0],\n",
    "                  '% post_grad': df['% post grad'].iloc[0],\n",
    "                  'unemployment_rate': df['Unemployment Rate'].iloc[0],\n",
    "                  'population': int(df['Population'].iloc[0]),\n",
    "                  'state': df['state'].iloc[0],\n",
    "                  'popln_of_state': int(df['Total population of state'].iloc[0]),\n",
    "                  'num_of_cluster1_patients': cluster_list[0],\n",
    "                  'num_of_cluster2_patients': cluster_list[1],\n",
    "                  'num_of_cluster3_patients': cluster_list[2],\n",
    "                  'num_of_cluster4_patients': cluster_list[3],\n",
    "                  'num_of_cluster5_patients': cluster_list[4]}\n",
    "        \n",
    "        df_zip = df_zip.append(zip_dictionary, ignore_index = True)\n",
    "        \n",
    "    df_zip.set_index('zipcode', inplace = True)\n",
    "    \n",
    "    for i in df_zip.columns[0:12]:\n",
    "        df_zip[i] = df_zip[i].astype('int16')\n",
    "        \n",
    "    df_zip['population'] = df_zip['population'].astype('int32')\n",
    "    df_zip['popln_of_state'] = df_zip['popln_of_state'].astype('int32')\n",
    "    \n",
    "    for i in df_zip.columns[24:]:\n",
    "        df_zip[i] = df_zip[i].astype('int16')\n",
    "    \n",
    "    joblib.dump(df_zip, path + '/df_zip.joblib')\n",
    "    \n",
    "    return df_zip\n",
    "\n",
    "\n",
    "# Function used to create a normalized version of the zipcode level dataframe\n",
    "def normalized_zipcode_df(df_zip):\n",
    "    \n",
    "    def normalizing(column, population, state_population):\n",
    "        # To bypass zero division error when population of that zipcode is 0\n",
    "        # we assume the population instead has a value of 1\n",
    "        if population == 0:\n",
    "            population = 1\n",
    "        return (column / population)\n",
    "    \n",
    "    normalized_columns = ['num_of_patients',\n",
    "                          'total_pd_generic_claims',\n",
    "                          'total_pd_BAOM_claims',\n",
    "                          'total_pd_saxenda_claims',\n",
    "                          'num_of_non_tiers',\n",
    "                          'num_of_tier_1s',\n",
    "                          'num_of_tier_2s',\n",
    "                          'num_of_tier_3s',\n",
    "                          'num_of_no_tiers',\n",
    "                          'num_of_saxenda_plantrak',\n",
    "                          'num_of_obese_patients',\n",
    "                          'num_of_cormobidity_patients',\n",
    "                          'num_of_cluster1_patients',\n",
    "                          'num_of_cluster2_patients',\n",
    "                          'num_of_cluster3_patients',\n",
    "                          'num_of_cluster4_patients',\n",
    "                          'num_of_cluster5_patients']\n",
    "    \n",
    "    df_normalized_zip = df_zip.copy()\n",
    "    for i in normalized_columns:\n",
    "        df_normalized_zip[i] = df_normalized_zip.apply(lambda x: normalizing(x[i], x['population'], x['popln_of_state']), axis = 1)\n",
    "        df_normalized_zip.rename(columns = {i: f'normalized_{i}'}, inplace = True)\n",
    "    \n",
    "    joblib.dump(df_normalized_zip, path + '/df_normalized_zip.joblib')\n",
    "    \n",
    "    return df_normalized_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zip = zipcode_df(zipcode_group)\n",
    "df_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zip.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zip = joblib.load(path + '/df_zip.joblib')\n",
    "df_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_zip = normalized_zipcode_df(df_zip)\n",
    "df_normalized_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_zip = joblib.load(path + '/df_normalized_zip.joblib')\n",
    "df_normalized_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the zipcodes in which number of patients does not exceed the zip code population\n",
    "df_normalized_zip[df_normalized_zip['normalized_num_of_patients'] <= 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_zip.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dataframes to excel files for further visualization analysis\n",
    "conversion(df_zip, 'Zipcode level statistics', excel = True)\n",
    "conversion(df_normalized_zip, 'Zipcode level Normalized statistics', excel = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and cleaning the zip code flat table before executing Machine learning clustering algorithm on it \n",
    "def cleaning_dataframe_zip(normalized_zip_df):\n",
    "    df_cleaned_zip = normalized_zip_df[normalized_zip_df['normalized_num_of_patients'] <= 1]\n",
    "    \n",
    "    df_cleaned_zip = df_cleaned_zip.drop(columns = ['population','state', 'popln_of_state', 'normalized_num_of_cluster1_patients', 'normalized_num_of_cluster2_patients',\n",
    "                                                      'normalized_num_of_cluster3_patients', 'normalized_num_of_cluster4_patients', 'normalized_num_of_cluster5_patients'])\n",
    "    \n",
    "    df_cleaned_zip = pd.DataFrame(MinMaxScaler().fit_transform(df_cleaned_zip), columns = df_cleaned_zip.columns, index = df_cleaned_zip.index)\n",
    "    # df_cleaned_zip = pd.DataFrame(StandardScaler().fit_transform(df_cleaned_zip), columns = df_cleaned_zip.columns, index = df_cleaned_zip.index)\n",
    "    \n",
    "    return df_cleaned_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_zip = cleaning_dataframe_zip(df_normalized_zip)\n",
    "df_cleaned_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_zip.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing techniques to speed up the process of calculating the SSE scores for K-Means elbow plot\n",
    "# and Bayesian Information Criterion Score for Gaussian Mixture Model\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = 8) as executor:\n",
    "        sse_zip_list = list(executor.map(kmeans_sse, [(df_cleaned_zip, k, 'N') for k in range(1, 16)]))\n",
    "        BIC_score_list = list(executor.map(bic_score, [(df_cleaned_zip, n, 'N') for n in range(1, 16)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbowplot(sse_zip_list, 1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_plot(BIC_score_list, 1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the Gaussian Mixture model on zip code flat table with 5 as our optimal number of groups\n",
    "# and saving the model in a serialized joblib format for future use\n",
    "gmm2 = gmm_alg(5, df_cleaned_zip)\n",
    "joblib.dump(gmm2, path + '/Gaussian Mixture Model for Clustering Zipcodes.joblib')\n",
    "predicted_groups = gmm2.predict(df_cleaned_zip) + 1\n",
    "predicted_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = pd.Series(dict(zip(df_cleaned_zip.index, predicted_groups)), name = 'Predicted Group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to create the zip code level group dictionary and the 'mean' and 'median' files for zip code level analysis\n",
    "def zipcode_level_results(df_dict, df_zipcode):\n",
    "    df_zipcode['Predicted Group'] = df_dict\n",
    "    df_zipcode['Predicted Group'] = df_zipcode['Predicted Group'].fillna(0).astype('int8')\n",
    "    df_zipcode['Predicted Group'] = df_zipcode['Predicted Group'].map({0: 'F', 1: 'A', 2: 'D', 3: 'B', 4: 'C', 5: 'E'})\n",
    "    \n",
    "    completed_df_dict = pd.Series(dict(zip(df_zipcode.index, df_zipcode['Predicted Group'])), name = 'Predicted Group')\n",
    "\n",
    "    df_means = df_zipcode.groupby('Predicted Group').mean()\n",
    "    df_median = df_zipcode.groupby('Predicted Group').median()\n",
    "    \n",
    "    return df_means, df_median, completed_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the normalized version of the mean and median files for zip code level analysis\n",
    "df_normalized_mean, df_normalized_median, completed_df_dict = zipcode_level_results(df_dict, df_normalized_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean, df_median, completed_df_dict = zipcode_level_results(df_dict, df_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the zip code level group dictionary in a serialized joblib format for future use\n",
    "joblib.dump(completed_df_dict, path + '/Zipcode and cluster group dictionary.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_df_dict = joblib.load(path + '/Zipcode and cluster group dictionary.joblib')\n",
    "completed_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the 'mean' and 'median' files to csv files\n",
    "conversion(df_normalized_mean, 'Zipcode level Grouping Means', excel = False)\n",
    "conversion(df_normalized_median, 'Zipcode Level Grouping Medians', excel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion(df_mean, 'Zipcode Level Non-Normalized Grouping Means', excel = False)\n",
    "conversion(df_median, 'Zipcode Level Non-Normalized Grouping Medians', excel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion(df_normalized_zip, 'Zipcode level Normalized statistics with Predicted Groups', excel = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion(df_zip, 'Zipcode level statistics with Predicted Groups', excel = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_zip.groupby('Predicted Group').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font size = \"12\">Combined Patient Level and Zipcode Level Results</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to display the overall results of the Patient and Zipcode level clustering process\n",
    "# The result is 30 sub-clusters in which a patient will be classified into\n",
    "# Also creating the combined patient level and zip code level dictionary\n",
    "def overall_results(df_patient_dict, df_zipcode_dict, df_cleaned):\n",
    "    df_patient_zip_level = joblib.load(path + '/df_patient_zip_level.joblib')\n",
    "    df_cleaned = df_cleaned.merge(df_patient_zip_level['zip'], left_on = 'patient_id', right_on = df_patient_zip_level.index, how = 'left')\n",
    "    df_cleaned = df_cleaned.merge(df_zipcode_dict, left_on = 'zip', right_on = df_zipcode_dict.index, how = 'left')\n",
    "    \n",
    "    df_cleaned.set_index('patient_id', inplace = True)\n",
    "    df_cleaned['Patient Cluster Ranking'] = df_patient_dict\n",
    "    df_cleaned.rename(columns = {'Predicted Group': 'Zip Code Group Ranking', 'zip': 'Zip Code'}, inplace = True)\n",
    "    df_cleaned['Sub-Cluster'] = df_cleaned['Patient Cluster Ranking'].astype('str') + '-' + df_cleaned['Zip Code Group Ranking']\n",
    "    \n",
    "    # Creating the final dictionary of patients with the Patient level ranking, Zipcode level ranking and the final sub-clusters\n",
    "    df_final_dict = df_cleaned[['Zip Code', 'Patient Cluster Ranking', 'Zip Code Group Ranking', 'Sub-Cluster']]\n",
    "    \n",
    "    df_cleaned.drop(columns = ['Patient Cluster Ranking', 'Zip Code', 'Zip Code Group Ranking'], inplace = True)\n",
    "    \n",
    "    df_final_mean = df_cleaned.groupby('Sub-Cluster').mean()\n",
    "    df_final_median = df_cleaned.groupby('Sub-Cluster').median()\n",
    "    \n",
    "    return df_final_dict, df_final_mean, df_final_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = joblib.load(path + '/df_cleaned.joblib')\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading both the patient level and zip code level cluster/group dictionary\n",
    "df_patient_dict = joblib.load(path + '/Patient ID and cluster group dictionary.joblib')\n",
    "df_zipcode_dict = joblib.load(path + '/Zipcode and cluster group dictionary.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_dict, df_final_mean, df_final_median = overall_results(df_patient_dict, df_zipcode_dict, df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the combined patient and zip code dictionary into a serialized joblib format\n",
    "joblib.dump(df_final_dict, path + '/Patient Complete Clustering dictionary.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_dict = joblib.load(path + '/Patient Complete Clustering dictionary.joblib')\n",
    "df_final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dataframe to a csv file\n",
    "conversion(df_final_dict.groupby('Sub-Cluster').size().to_frame().rename(columns = {0: 'Number of patients in the sub-cluster'}), 'Sizes of Sub-Cluster', excel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the final mean and median files for each patient grouped into the 30 sub-clusters into csv files\n",
    "conversion(df_final_mean, 'Final Patient Sub-Clusters Means', excel = False)\n",
    "conversion(df_final_median, 'Final Patient Sub-Clusters Medians', excel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_staytime = joblib.load(path + '/df_staytime.joblib')\n",
    "df_staytime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_staytime['Sub-Cluster'] = df_final_dict['Sub-Cluster']\n",
    "df_staytime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the stay-time data for each sub-cluster\n",
    "staytime_sub_cluster_means = df_staytime.groupby('Sub-Cluster').mean()\n",
    "staytime_sub_cluster_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion(staytime_sub_cluster_means, 'Stay-time Sub-Cluster Statistics', excel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font size = \"12\">Visualization and Statistics</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_dict = joblib.load(path + '/Patient Complete Clustering dictionary.joblib')\n",
    "df_final_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socioeconomic_df = joblib.load(path + '/socioeconomic data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing this specific dataframe to load in order to visualize some categorical variables that is not easily visualized\n",
    "# in the 'mean' or 'median' files\n",
    "df_merge_cluster = pd.read_sql_query(q_str_col_9, engine_laad)\n",
    "df_merge_cluster = df_merge_cluster.merge(socioeconomic_df[['Population', 'Total population of state']], left_on = 'zip', right_on = socioeconomic_df.index, how = 'left')\n",
    "df_merge_cluster = df_merge_cluster.merge(df_final_dict, left_on = 'patient_id', right_on = df_final_dict.index, how = 'left')\n",
    "df_merge_cluster.set_index('patient_id', inplace = True)\n",
    "df_merge_cluster['nni_saxenda_gsb'] = np.where(df_merge_cluster['nni_saxenda_gsb'] == '', 'Non-Tiers', df_merge_cluster['nni_saxenda_gsb'])\n",
    "df_merge_cluster.dropna(subset = ['Patient Cluster Ranking'], inplace = True)\n",
    "df_merge_cluster['Patient Cluster Ranking'] = df_merge_cluster['Patient Cluster Ranking'].astype('int8')\n",
    "df_merge_cluster.drop(columns = ['joined_prescriber_id', 'joined_plantrak_id'], inplace = True)\n",
    "df_merge_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Patient Level Visualizations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function is used to create a dictionary with the Cluster group as the key and the dataframe as the value\n",
    "# Default grouped column is the Patient Cluster Ranking\n",
    "def cluster_grouping(df_merge_cluster, grouped_column = 'Patient Cluster Ranking'):\n",
    "    \n",
    "    # Label encoding yn columns to visualize it in graphs easier\n",
    "    for i in df_merge_cluster.dtypes[df_merge_cluster.dtypes == 'O'].index:\n",
    "        if i[-2:] == 'yn': \n",
    "            df_merge_cluster[i] = df_merge_cluster[i].map({'Y': 1, 'N': 0})\n",
    "    \n",
    "    # Groupby the entire database to clusters for analysis on each cluster\n",
    "    cluster_groups = df_merge_cluster.groupby(grouped_column)\n",
    "    \n",
    "    # Create a dictionary to access each dataframe for each cluster easier\n",
    "    cluster_group_dict = dict()\n",
    "    for key, df in cluster_groups:\n",
    "        cluster_group_dict[key] = df\n",
    "        \n",
    "    return (cluster_group_dict, cluster_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(patient_level_cluster_group_dict, patient_level_cluster_groups) = cluster_grouping(df_merge_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to plot the percentage of the desired categorical variable in each cluster\n",
    "# Default value of selected categorical varibale is Saxenda Tier prescribers\n",
    "def pct_categorical_variable_group(df_merge_cluster, cluster_groups, grouped_column = 'Patient Cluster Ranking', categorical_variable = 'nni_saxenda_gsb', fig_name = 'Saxenda Tier prescribers', y_label = None, fig_size = (10, 10), color = None):\n",
    "    categorical_cluster_group = df_merge_cluster.groupby([grouped_column, categorical_variable])\n",
    "    cluster_size_dict = cluster_groups.size().to_dict()\n",
    "    \n",
    "    categorical_stats = categorical_cluster_group.size().unstack()\n",
    "    \n",
    "    for i, j in enumerate(cluster_size_dict):\n",
    "        categorical_stats.iloc[i] = categorical_stats.iloc[i].apply(lambda x: (x / cluster_size_dict[j]) * 100)\n",
    "    \n",
    "    %matplotlib inline\n",
    "    sns.set()\n",
    "    categorical_stats.plot(y = y_label, title = f'Percentage of {fig_name}', kind = 'bar', figsize = fig_size, colormap = color)\n",
    "    plt.legend(loc = 'center left', bbox_to_anchor = (1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster, patient_level_cluster_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster, patient_level_cluster_groups, categorical_variable = 'method_of_payment', fig_name = 'Different Insurance plans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL']['model_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'], \n",
    "                               cluster_grouping(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'])[1], \n",
    "                               categorical_variable = 'model_type', \n",
    "                               fig_name = 'Different Commerical Insurance models',\n",
    "                               y_label = ['EMPLOYER', 'PBM', 'PPO', 'IPA', 'COMBO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to visulalize the top 5 states that each patient resides in each cluster,\n",
    "# both as a normalized and the actual population\n",
    "def state_visualization(cluster_group_dict):\n",
    "    state_group = list()\n",
    "    new_state_group = list()\n",
    "    \n",
    "    for value in cluster_group_dict.values():\n",
    "        state_group.append(value.groupby(['state', 'Total population of state']).size())\n",
    "        \n",
    "    for group in state_group:\n",
    "        group = group.to_frame()\n",
    "        group.rename(columns = {0: 'num of patients'}, inplace = True)\n",
    "        group.reset_index(inplace = True)\n",
    "        group['normalized num of patients'] = group['num of patients'] / group['Total population of state']\n",
    "        group.set_index('state', inplace = True)\n",
    "        new_state_group.append(group)\n",
    "    \n",
    "    %matplotlib inline\n",
    "    sns.set()\n",
    "    \n",
    "    ROW_NUM = len(cluster_group_dict) \n",
    "    COL_NUM = 2\n",
    "    fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize = (9, 9))\n",
    "\n",
    "    i = 0\n",
    "    for (key, values) in enumerate(new_state_group):\n",
    "        ax = axes[int(i / COL_NUM), i % COL_NUM]\n",
    "        normalized_sizes = new_state_group[key]['normalized num of patients'].sort_values(ascending = False).head()\n",
    "        normalized_sizes.plot(kind = 'barh', ax = ax)\n",
    "        ax.set_title(key + 1)\n",
    "        ax.set_xlabel('normalized sizes')\n",
    "        i += 1\n",
    "\n",
    "    for (key, values) in enumerate(new_state_group):\n",
    "        ax = axes[int(i / COL_NUM), i % COL_NUM]\n",
    "        sizes = new_state_group[key]['num of patients'].sort_values(ascending = False).head()\n",
    "        sizes.plot(kind = 'barh', ax = ax)\n",
    "        ax.set_title(key + 1)\n",
    "        ax.set_xlabel('regular sizes')\n",
    "        i += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "        \n",
    "    return new_state_group\n",
    "\n",
    "# Function used to to visualize the top 10 zipcodes in each cluster, both as a normalized and the\n",
    "# actual population\n",
    "def zip_visualization(cluster_group_dict):\n",
    "    state_group = list()\n",
    "    new_state_group = list()\n",
    "    \n",
    "    for value in cluster_group_dict.values():\n",
    "        state_group.append(value.groupby(['zip', 'Population']).size())\n",
    "        \n",
    "    for group in state_group:\n",
    "        group = group.to_frame()\n",
    "        group.rename(columns = {0: 'num of patients'}, inplace = True)\n",
    "        group.reset_index(inplace = True)\n",
    "        group['normalized num of patients'] = group['num of patients'] / group['Population']\n",
    "        group = group[group['normalized num of patients'] != np.inf]\n",
    "        group = group[group['normalized num of patients'] <= 1]\n",
    "        group.set_index('zip', inplace = True)\n",
    "        new_state_group.append(group)\n",
    "    \n",
    "    %matplotlib inline\n",
    "    sns.set()\n",
    "\n",
    "    \n",
    "    ROW_NUM = len(cluster_group_dict) - 1\n",
    "    COL_NUM = 3\n",
    "    \n",
    "    fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize = (9, 9))\n",
    "\n",
    "    i = 0\n",
    "    for (key, values) in enumerate(new_state_group):\n",
    "        ax = axes[int(i / COL_NUM), i % COL_NUM]\n",
    "        normalized_sizes = values['normalized num of patients'].sort_values(ascending = False).iloc[0:10]\n",
    "        normalized_sizes.plot(kind = 'barh', ax = ax)\n",
    "        ax.set_title(key + 1)\n",
    "        ax.set_xlabel('normalized sizes')\n",
    "        i += 1\n",
    "\n",
    "    for (key, values) in enumerate(new_state_group):\n",
    "        ax = axes[int(i / COL_NUM), i % COL_NUM]\n",
    "        sizes = values['num of patients'].sort_values(ascending = False).iloc[0:10]\n",
    "        sizes.plot(kind = 'barh', ax = ax)\n",
    "        ax.set_title(key + 1)\n",
    "        ax.set_xlabel('regular sizes')\n",
    "        i += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "        \n",
    "    return new_state_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sizes = state_visualization(patient_level_cluster_group_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_sizes = zip_visualization(patient_level_cluster_group_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the top 50 percentile zip codes given to us by the Obesity Insights and Analytics team\n",
    "try:\n",
    "    Top_50_percentile_df = joblib.load(path + '/Top 50 Percentile Zip Code.joblib')\n",
    "except FileNotFoundError:\n",
    "    Top_50_percentile_df = pd.read_excel(path + '/Top 50 Percentile Zip Code.xlsx')\n",
    "    Top_50_percentile_df['Zip'] = Top_50_percentile_df['Zip'].apply(lambda x: '00' + str(x) if len(str(x)) == 3 else ('0' + str(x) if len(str(x)) == 4 else str(x)))\n",
    "    Top_50_percentile_df.set_index('Zip', inplace = True)\n",
    "    Top_50_percentile_df = Top_50_percentile_df.merge(socioeconomic_df['Population'], left_on = Top_50_percentile_df.index, right_on = socioeconomic_df.index, how = 'left')\n",
    "    Top_50_percentile_df.rename(columns = {'key_0': 'ZipCode'}, inplace = True)\n",
    "    joblib.dump(Top_50_percentile_df, path + '/Top 50 Percentile Zip Code.joblib')\n",
    "    \n",
    "Top_50_percentile_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to see the percentage of overlap of the Top 4000 zipcodes ranked by normalized number of patients with that of \n",
    "# DTC Saxenda Target List analysis conducted by Obesity Insights and Analytics team in 2018\n",
    "def overlap_stats(zip_sizes):\n",
    "    top_4000_zip = dict()\n",
    "    percentage_of_overlap = dict()\n",
    "    \n",
    "    for i in range(len(zip_sizes)):\n",
    "        top_4000_zip[i] = zip_sizes[i].sort_values(by = ['normalized num of patients'], ascending = False).iloc[0:4000]\n",
    "        \n",
    "    Joined_top_50_clusters = Top_50_percentile_df\n",
    "    for i in range(len(top_4000_zip)):\n",
    "        Joined_top_50_clusters = Joined_top_50_clusters.merge(top_4000_zip[i][['num of patients', 'normalized num of patients']], left_on = 'ZipCode', right_on = 'zip', how = 'left')\n",
    "        Joined_top_50_clusters.rename(columns = {'num of patients': f'Cluster {i} - num of patients', 'normalized num of patients': f'Cluster {i} - normalized num of patients'}, inplace = True)\n",
    "\n",
    "    Joined_top_50_clusters.set_index('ZipCode', inplace = True)\n",
    "    \n",
    "    percentage_of_overlap = dict()\n",
    "\n",
    "    for i in range(len(zip_sizes)):\n",
    "        percentage_of_overlap[i + 1] = (len(Joined_top_50_clusters) - Joined_top_50_clusters[f'Cluster {i} - num of patients'].isna().sum()) / len(Joined_top_50_clusters) * 100\n",
    "        \n",
    "    return percentage_of_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_percentage = overlap_stats(zip_sizes)\n",
    "overlap_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Zipcode visualizations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the zip code level dictionary for each zip code group\n",
    "(zip_level_cluster_group_dict, zip_level_cluster_groups) = cluster_grouping(df_merge_cluster, grouped_column = 'Zip Code Group Ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster, zip_level_cluster_groups, grouped_column = 'Zip Code Group Ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster, zip_level_cluster_groups, grouped_column = 'Zip Code Group Ranking', categorical_variable = 'method_of_payment', fig_name = 'Different Insurance plans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'], \n",
    "                               cluster_grouping(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'], grouped_column = 'Zip Code Group Ranking')[1], \n",
    "                               grouped_column = 'Zip Code Group Ranking',\n",
    "                               categorical_variable = 'model_type', \n",
    "                               fig_name = 'Different Commerical Insurance models',\n",
    "                               y_label = ['EMPLOYER', 'PBM', 'PPO', 'IPA', 'COMBO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sub-Cluster visualizations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the sub-cluster level dictionary for each sub-cluster\n",
    "(sub_cluster_group_dict, sub_cluster_groups) = cluster_grouping(df_merge_cluster, grouped_column = 'Sub-Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster, sub_cluster_groups, grouped_column = 'Sub-Cluster', y_label = ['Tier 1'], fig_size = (12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster, sub_cluster_groups, grouped_column = 'Sub-Cluster', y_label = ['Tier 2'], fig_size = (12, 12), color = 'Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster, sub_cluster_groups, grouped_column = 'Sub-Cluster', y_label = ['Tier 3'], fig_size = (12, 12), color = 'RdGy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster, sub_cluster_groups, grouped_column = 'Sub-Cluster', y_label = ['Non-Tiers'], fig_size = (12, 12), color = 'seismic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster, \n",
    "                               sub_cluster_groups, \n",
    "                               grouped_column = 'Sub-Cluster', \n",
    "                               categorical_variable = 'method_of_payment', \n",
    "                               fig_name = 'Different Insurance plans', \n",
    "                               y_label = ['COMMERCIAL'], \n",
    "                               fig_size = (12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster, \n",
    "                               sub_cluster_groups, \n",
    "                               grouped_column = 'Sub-Cluster', \n",
    "                               categorical_variable = 'method_of_payment', \n",
    "                               fig_name = 'Different Insurance plans', \n",
    "                               y_label = ['ASSISTANCE'], \n",
    "                               fig_size = (12, 12), \n",
    "                               color = 'Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster, \n",
    "                               sub_cluster_groups, \n",
    "                               grouped_column = 'Sub-Cluster', \n",
    "                               categorical_variable = 'method_of_payment', \n",
    "                               fig_name = 'Different Insurance plans', \n",
    "                               y_label = ['CASH'], \n",
    "                               fig_size = (12, 12), \n",
    "                               color = 'PuOr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'], \n",
    "                               cluster_grouping(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'], grouped_column = 'Sub-Cluster')[1], \n",
    "                               grouped_column = 'Sub-Cluster',\n",
    "                               categorical_variable = 'model_type', \n",
    "                               fig_name = 'Different Commerical Insurance models',\n",
    "                               y_label = ['EMPLOYER'],\n",
    "                               fig_size + (12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'], \n",
    "                               cluster_grouping(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'], grouped_column = 'Sub-Cluster')[1], \n",
    "                               grouped_column = 'Sub-Cluster',\n",
    "                               categorical_variable = 'model_type', \n",
    "                               fig_name = 'Different Commerical Insurance models',\n",
    "                               y_label = ['PBM'],\n",
    "                               fig_size = (12, 12),\n",
    "                               color = 'Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'], \n",
    "                               cluster_grouping(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'], grouped_column = 'Sub-Cluster')[1], \n",
    "                               grouped_column = 'Sub-Cluster',\n",
    "                               categorical_variable = 'model_type', \n",
    "                               fig_name = 'Different Commerical Insurance models',\n",
    "                               y_label = ['PPO'],\n",
    "                               fig_size = (12, 12),\n",
    "                               color = 'PuOr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_categorical_variable_group(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'], \n",
    "                               cluster_grouping(df_merge_cluster[df_merge_cluster['method_of_payment'] == 'COMMERCIAL'], grouped_column = 'Sub-Cluster')[1], \n",
    "                               grouped_column = 'Sub-Cluster',\n",
    "                               categorical_variable = 'model_type', \n",
    "                               fig_name = 'Different Commerical Insurance models',\n",
    "                               y_label = ['IPA'],\n",
    "                               fig_size = (12, 12),\n",
    "                               color = 'seismic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Heatmap Analysis</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to display a heatmap of the number of patients in each cluster across all states \n",
    "def heatmap_number_of_patients(cluster_group_dict):\n",
    "\n",
    "    for i in range(len(cluster_group_dict)):\n",
    "        fig = go.Figure(data = go.Choropleth(\n",
    "            locations = cluster_group_dict[i].groupby('state').size().index, # Spatial coordinates\n",
    "            z = cluster_group_dict[i].groupby('state').size(), # Data to be color-coded\n",
    "            locationmode = 'USA-states', # set of locations match entries in `locations`\n",
    "            colorscale = 'Blues',\n",
    "            colorbar_title = \"Number of patients\",\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = 'Number of patients - Cluster ' + str(i),\n",
    "            geo_scope='usa', # limit map scope to USA\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to display a heatmap of the number of patients normalized against the total population\n",
    "# of each state in each cluster across all states \n",
    "def heatmap_number_of_patients_normalized(cluster_group_dict):\n",
    "\n",
    "    for i in range(len(cluster_group_dict)):\n",
    "        fig = go.Figure(data = go.Choropleth(\n",
    "            locations = cluster_group_dict[i].groupby('state').size().index, # Spatial coordinates\n",
    "            z = cluster_group_dict[i].groupby(['state', 'Total population of state']).size().reset_index().rename(columns = {0: 'size'}).set_index('state')['size'] \n",
    "                / cluster_group_dict[i].groupby(['state', 'Total population of state']).size().reset_index().set_index('state')['Total population of state'], # Data to be color-coded\n",
    "            locationmode = 'USA-states', # set of locations match entries in `locations`\n",
    "            colorscale = 'Blues',\n",
    "            colorbar_title = \"ratio\",\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = 'Normalized Number of patients - Cluster ' + str(i),\n",
    "            geo_scope ='usa', # limit map scope to USA\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to display a heatmap of the mean of a chosen dimension in each cluster across all states \n",
    "# Default value of dimension is total_pd_branded_aom_claims\n",
    "def heatmap_mean_of_dimension(cluster_group_dict, dimension = 'total_pd_branded_aom_claims'):\n",
    "\n",
    "    for i in range(len(cluster_group_dict)):\n",
    "        fig = go.Figure(data = go.Choropleth(\n",
    "            locations = cluster_group_dict[i].groupby('state').mean()[dimension].index, # Spatial coordinates\n",
    "            z = cluster_group_dict[i].groupby('state').mean()[dimension], # Data to be color-coded\n",
    "            locationmode = 'USA-states', # set of locations match entries in `locations`\n",
    "            colorscale = 'Blues',\n",
    "            colorbar_title = dimension,\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = 'Mean of ' + dimension + ' - Cluster ' + str(i),\n",
    "            geo_scope='usa', # limit map scope to USA\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to display a heatmap of the mean of a chosen dimension normalized against the total population\n",
    "# of each state in each cluster across all states \n",
    "# Default value of dimension is total_pd_branded_aom_claims\n",
    "def heatmap_mean_of_dimension_normalized(cluster_group_dict, dimension = 'total_pd_branded_aom_claims'):\n",
    "\n",
    "    for i in range(len(cluster_group_dict)):\n",
    "        fig = go.Figure(data = go.Choropleth(\n",
    "            locations = cluster_group_dict[i].groupby('state').mean()[dimension].index, # Spatial coordinates\n",
    "            z = cluster_group_dict[i].groupby('state').mean()[dimension] \n",
    "                / cluster_group_dict[i].groupby('state').mean()['Total population of state'], # Data to be color-coded\n",
    "            locationmode = 'USA-states', # set of locations match entries in `locations`\n",
    "            colorscale = 'Blues',\n",
    "            colorbar_title = \"Normalized \" + dimension,\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text = 'Normalized Mean of ' + dimension + ' - Cluster ' + str(i),\n",
    "            geo_scope='usa', # limit map scope to USA\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font size = \"12\">Patient Sub-Cluster Ranking and Categorization</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the complete patient and zip code combined dictionary\n",
    "df_final_dict = joblib.load(path + '/Patient Complete Clustering dictionary.joblib')\n",
    "df_final_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_dict['Zip Code Group Ranking'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the ranking model of the top 15 sub-clusters based on the visualizations\n",
    "df_final_dict['Overall Ranking'] = df_final_dict['Sub-Cluster'].map({'1-A': '1',\n",
    "                                                                     '1-F': '2',\n",
    "                                                                     '1-B': '3',\n",
    "                                                                     '1-D': '4',\n",
    "                                                                     '2-A': '5',\n",
    "                                                                     '2-F': '6',\n",
    "                                                                     '2-B': '7',\n",
    "                                                                     '2-D': '8',\n",
    "                                                                     '5-A': '9',\n",
    "                                                                     '5-D': '10',\n",
    "                                                                     '5-B': '11',\n",
    "                                                                     '3-A': '12',\n",
    "                                                                     '3-B': '13',\n",
    "                                                                     '3-D': '14',\n",
    "                                                                     '3-F': '15'})\n",
    "# The bottom 15 sub-clusters are labelled 'Unranked'\n",
    "df_final_dict['Overall Ranking'].fillna('Unranked', inplace = True)\n",
    "df_final_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the categories for each similar ranking groups\n",
    "df_final_dict['Category'] = df_final_dict['Overall Ranking'].map({'1': 'NNI AOM Enthusiasts',\n",
    "                                                                  '2': 'NNI AOM Enthusiasts',\n",
    "                                                                  '3': 'NNI AOM Enthusiasts',\n",
    "                                                                  '4': 'NNI AOM Enthusiasts',\n",
    "                                                                  '5': 'NNI AOM Convertibles',\n",
    "                                                                  '6': 'NNI AOM Convertibles',\n",
    "                                                                  '7': 'NNI AOM Convertibles',\n",
    "                                                                  '8': 'NNI AOM Convertibles',\n",
    "                                                                  '9': 'NNI AOM Potentials',\n",
    "                                                                  '10': 'NNI AOM Potentials',\n",
    "                                                                  '11': 'NNI AOM Potentials',\n",
    "                                                                  '12': 'NNI AOM Rejects',\n",
    "                                                                  '13': 'NNI AOM Rejects',\n",
    "                                                                  '14': 'NNI AOM Rejects',\n",
    "                                                                  '15': 'NNI AOM Rejects',\n",
    "                                                                  'Unranked': 'NNI AOM Hopeless'})\n",
    "df_final_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this complete patient level and zip code level dictionary, and ranking model into one serialized joblib file for future use\n",
    "joblib.dump(df_final_dict, path + '/Patient Complete Clustering dictionary.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font size = \"12\">Files for Zipcode Heatmap Visualization on Tableau</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the complete dictionary with the ranking model\n",
    "df_final_dict = joblib.load(path + '/Patient Complete Clustering dictionary.joblib')\n",
    "df_final_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_dict['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socioeconomic_df = joblib.load(path + '/socioeconomic data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a specific patient dataset and combining it with socioeconomic dataset to get patients and zip code data \n",
    "# and appending their corresponding category based on the complete ranking model dictionary that was loaded\n",
    "df_merge_cluster = pd.read_sql_query(q_str_col_9, engine_laad)\n",
    "df_merge_cluster = df_merge_cluster.merge(socioeconomic_df[['Population', 'Total population of state']], left_on = 'zip', right_on = socioeconomic_df.index, how = 'left')\n",
    "df_merge_cluster = df_merge_cluster.merge(df_final_dict, left_on = 'patient_id', right_on = df_final_dict.index, how = 'left')\n",
    "df_merge_cluster.set_index('patient_id', inplace = True)\n",
    "df_merge_cluster['nni_saxenda_gsb'] = np.where(df_merge_cluster['nni_saxenda_gsb'] == '', 'Non-Tiers', df_merge_cluster['nni_saxenda_gsb'])\n",
    "df_merge_cluster.dropna(subset = ['Patient Cluster Ranking'], inplace = True)\n",
    "df_merge_cluster['Patient Cluster Ranking'] = df_merge_cluster['Patient Cluster Ranking'].astype('int8')\n",
    "df_merge_cluster.drop(columns = ['joined_prescriber_id', 'joined_plantrak_id'], inplace = True)\n",
    "df_merge_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_group = df_merge_cluster.groupby('Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframes for each category\n",
    "df_enthusiasts = category_group.get_group('NNI AOM Enthusiasts')\n",
    "df_convertibles = category_group.get_group('NNI AOM Convertibles')\n",
    "df_potentials = category_group.get_group('NNI AOM Potentials')\n",
    "df_rejects = category_group.get_group('NNI AOM Rejects')\n",
    "df_hopeless = category_group.get_group('NNI AOM Hopeless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dataframes to csv files for heatmap analysis using Tableau\n",
    "conversion(df_enthusiasts, 'NNI AOM Enthusiasts', excel = False)\n",
    "conversion(df_convertibles, 'NNI AOM Convertibles', excel = False)\n",
    "conversion(df_potentials, 'NNI AOM Potentials', excel = False)\n",
    "conversion(df_rejects, 'NNI AOM Rejects', excel = False)\n",
    "conversion(df_hopeless, 'NNI AOM Hopeless', excel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion(df_final_dict, 'All NNI AOM categories', excel = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font size = \"12\">Patient Classification Network</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the combined dictionary with the ranking model\n",
    "df_final_dict = joblib.load(path + '/Patient Complete Clustering dictionary.joblib')\n",
    "df_final_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used ta create nodes and edges of the Patient Classification Network Ranking system\n",
    "# In order to visualize our classification model as a whole\n",
    "def nodes_edges(df_final_dict):\n",
    "    patient_level_nodes = sorted(df_final_dict['Patient Cluster Ranking'].unique().astype('str'))\n",
    "    zipcode_level_nodes = sorted(df_final_dict['Sub-Cluster'].unique())\n",
    "    \n",
    "    nodes = ['28 Million Patients']\n",
    "    nodes.extend(patient_level_nodes)\n",
    "    nodes.extend(zipcode_level_nodes)\n",
    "    \n",
    "    ranking_nodes = list() \n",
    "    for i in df_final_dict['Overall Ranking'].unique():\n",
    "        if i != 'Unranked':\n",
    "            ranking_nodes.append(f'Rank {i}')\n",
    "        else:\n",
    "            ranking_nodes.append(i)\n",
    "            \n",
    "    category_nodes = df_final_dict['Category'].unique()\n",
    "    \n",
    "    patient_level_sizes = df_final_dict.groupby('Patient Cluster Ranking').size().to_dict()\n",
    "    for i in patient_level_sizes:\n",
    "        patient_level_sizes[i] = '{:,}'.format(patient_level_sizes[i])\n",
    "        patient_level_sizes[i] = patient_level_sizes[i].replace(',', \" \")\n",
    "    \n",
    "    zipcode_level_sizes = df_final_dict.groupby('Sub-Cluster').size().to_dict()\n",
    "    for i in zipcode_level_sizes:\n",
    "        zipcode_level_sizes[i] = \"{:,}\".format(zipcode_level_sizes[i])\n",
    "        zipcode_level_sizes[i] = zipcode_level_sizes[i].replace(',', \" \")\n",
    "    \n",
    "    root_patient_edges = [(nodes[0], nodes[i], patient_level_sizes[i]) for i in range(1, len(patient_level_nodes) + 1)]\n",
    "    patient_zip_edges = list(zip([i[0] for i in zipcode_level_nodes], zipcode_level_nodes, zipcode_level_sizes.values()))\n",
    "    \n",
    "    sub_cluster_rank_edges = [('1-A', 'Rank 1'),\n",
    "                              ('1-F', 'Rank 2'),\n",
    "                              ('1-B', 'Rank 3'),\n",
    "                              ('1-D', 'Rank 4'),\n",
    "                              ('2-A', 'Rank 5'),\n",
    "                              ('2-F', 'Rank 6'),\n",
    "                              ('2-B', 'Rank 7'),\n",
    "                              ('2-D', 'Rank 8'),\n",
    "                              ('5-A', 'Rank 9'),\n",
    "                              ('5-D', 'Rank 10'),\n",
    "                              ('5-B', 'Rank 11'),\n",
    "                              ('3-A', 'Rank 12'),\n",
    "                              ('3-B', 'Rank 13'),\n",
    "                              ('3-D', 'Rank 14'),\n",
    "                              ('3-F', 'Rank 15')]\n",
    "    \n",
    "    for i in zipcode_level_nodes:\n",
    "        if i not in list(zip(*sub_cluster_rank_edges))[0]:\n",
    "            sub_cluster_rank_edges.append((i, 'Unranked'))\n",
    "            \n",
    "    cluster_category_edges = list()\n",
    "    for i in range(len(ranking_nodes)):\n",
    "        if i + 1 <= 4:\n",
    "            cluster_category_edges.append((f'Rank {i + 1}', 'NNI AOM Enthusiasts'))\n",
    "        elif (i + 1 >= 5) and (i + 1 <= 8):\n",
    "            cluster_category_edges.append((f'Rank {i + 1}', 'NNI AOM Convertibles'))\n",
    "        elif (i + 1 >= 9) and (i + 1 <= 11):\n",
    "            cluster_category_edges.append((f'Rank {i + 1}', 'NNI AOM Potentials'))\n",
    "        elif (i + 1 >= 12) and (i + 1 <= 15):\n",
    "            cluster_category_edges.append((f'Rank {i + 1}', 'NNI AOM Rejects'))\n",
    "        else:\n",
    "            cluster_category_edges.append(('Unranked', 'NNI AOM Hopeless'))\n",
    "    \n",
    "    return (nodes, patient_level_nodes, zipcode_level_nodes, ranking_nodes, \n",
    "            category_nodes, root_patient_edges, patient_zip_edges, sub_cluster_rank_edges, cluster_category_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clustering_nodes, patient_level_nodes, zipcode_level_nodes, ranking_nodes, \n",
    " category_nodes, root_patient_edges, patient_zip_edges, sub_cluster_rank_edges, cluster_category_edges) = nodes_edges(df_final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_revenue_edges = [('NNI AOM Convertibles', 'Potential Revenue of $17 Billion', 'Marketing Campaigns'),\n",
    "                          ('NNI AOM Potentials', 'Potential Revenue of $17 Billion', 'Investment Efforts')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the patient network and creating the nodes and edges below\n",
    "patient_network = pydotplus.Dot(graph_type = 'digraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {'1': 'coral1',\n",
    "              '2': 'orange',\n",
    "              '3': 'yellow',\n",
    "              '4': 'green',\n",
    "              '5': 'cyan'}\n",
    "\n",
    "color_func = lambda x: color_dict[x]\n",
    "\n",
    "for n in clustering_nodes:\n",
    "    if n == '28 Million Patients':\n",
    "        color = 'white'\n",
    "    elif n in patient_level_nodes:\n",
    "        color = 'gray'\n",
    "    else:\n",
    "        color = color_func(n[0])\n",
    "    node = pydotplus.Node(n, style = 'filled', fillcolor = color)\n",
    "    patient_network.add_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in ranking_nodes:\n",
    "    node = pydotplus.Node(n, style = 'filled', fillcolor = 'pink')\n",
    "    patient_network.add_node(node)\n",
    "\n",
    "for n in category_nodes:\n",
    "    node = pydotplus.Node(n, style = 'filled', fillcolor = 'olivedrab1')\n",
    "    patient_network.add_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = pydotplus.Node('Potential Revenue of $17 Billion', style = 'filled', fillcolor = 'midnightblue', fontcolor = 'white')\n",
    "patient_network.add_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in root_patient_edges:\n",
    "    edge = pydotplus.Edge(e[0], e[1], label = e[2], labelfontsize = 9.0)\n",
    "    patient_network.add_edge(edge)\n",
    "\n",
    "for e in patient_zip_edges:\n",
    "    edge = pydotplus.Edge(e[0], e[1], label = e[2], labelfontsize = 9.0)\n",
    "    patient_network.add_edge(edge)\n",
    "\n",
    "for e in sub_cluster_rank_edges:\n",
    "    edge = pydotplus.Edge(e[0], e[1])\n",
    "    patient_network.add_edge(edge)\n",
    "\n",
    "for e in cluster_category_edges:\n",
    "    edge = pydotplus.Edge(e[0], e[1])\n",
    "    patient_network.add_edge(edge)\n",
    "    \n",
    "for e in category_revenue_edges:\n",
    "    edge = pydotplus.Edge(e[0], e[1], label = e[2])\n",
    "    patient_network.add_edge(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying how the patient network looks like\n",
    "im = Image(patient_network.create_jpg())\n",
    "display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a more simpified version of the patient level network with the nodes and edges below\n",
    "patient_network2 = pydotplus.Dot(graph_type = 'digraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_category_edges = [('28 Million Patients', i) for i in category_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict2 = {'NNI AOM Enthusiasts': ['lightsteelblue1', 'black'],\n",
    "               'NNI AOM Convertibles': ['midnightblue', 'white'],\n",
    "               'NNI AOM Potentials': ['midnightblue', 'white'],\n",
    "               'NNI AOM Rejects': ['lightsteelblue1', 'black'],\n",
    "               'NNI AOM Hopeless': ['orangered', 'white']}\n",
    "\n",
    "color_func2 = lambda x: color_dict2[x]\n",
    "\n",
    "root_node = pydotplus.Node(clustering_nodes[0])\n",
    "patient_network2.add_node(root_node)\n",
    "\n",
    "for i in category_nodes:\n",
    "    fill_color = color_func2(i)[0]\n",
    "    font_color = color_func2(i)[1]\n",
    "    node = pydotplus.Node(i, style = 'filled', fillcolor = fill_color, fontcolor = font_color)\n",
    "    patient_network2.add_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = pydotplus.Node('Potential Revenue of $17 Billion', style = 'filled', fillcolor = 'azure', fontcolor = 'black')\n",
    "patient_network2.add_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in root_category_edges:\n",
    "    edge = pydotplus.Edge(e[0], e[1])\n",
    "    patient_network2.add_edge(edge)\n",
    "    \n",
    "for e in category_revenue_edges:\n",
    "    edge = pydotplus.Edge(e[0], e[1], label = e[2])\n",
    "    patient_network2.add_edge(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying how the simplified patient network looks like\n",
    "im = Image(patient_network2.create_jpg())\n",
    "display(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
